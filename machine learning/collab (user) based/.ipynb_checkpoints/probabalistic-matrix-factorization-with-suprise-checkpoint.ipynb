{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "import os\n",
    "# print(os.listdir(\"../input\"))\n",
    "\n",
    "import surprise  #Scikit-Learn library for recommender systems. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "Creating an Explicit Latent Matrix Factorization Recommender System for Books10k Dataset.\n",
    "We're basing this off of this paper: https://papers.nips.cc/paper/3208-probabilistic-matrix-factorization.pdf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Surprise, Data Wrangling is almost automated. We'll load the dataset into Pandas, then leverage the surprise package "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probabilistic Matrix Factorization is a prediction method where you estimate the best factorization for the Ratings Matrix. Taking an example, lets say there's a rating matrix of 100k users and 10k items. We'll try to factor this dataset into its latent factors. Think of PCA or Singular Value Decompisition. That would be great to perform, but we have a huge issue. Our dataset is FILLED with NANs. This means that SVD and PCA is undefined, and so instead, we'll have to factor this differently. Primarily, we'll initialize two random matrices, one which estimates the altent factors for the user, and one which estimates the latent factors for the items. \n",
    "\n",
    "In our example, we initialize a 100k by 40 matrix, to get 40 latent factors out of the 100k users we have, and a 40 by 10k matrix, which estimates the latent factors of our matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw=pd.read_csv('./temp_data/rating.csv')\n",
    "# raw.drop_duplicates(inplace=True)\n",
    "print('we have',raw.shape[0], 'ratings')\n",
    "print('the number of unique users we have is:', len(raw.user_id.unique()))\n",
    "print('the number of unique books we have is:', len(raw.book_id.unique()))\n",
    "print(\"The median user rated %d books.\"%raw.user_id.value_counts().median())\n",
    "print('The max rating is: %d'%raw.rating.max(),\"the min rating is: %d\"%raw.rating.min())\n",
    "raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading dataset into Surprise:\n",
    "For a dataset to be loaded into surprise from a pandas dataframe, you have to specify a reader object and ensure that the columns the dataset are in the order: user,item, rating. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#swapping columns\n",
    "raw=raw[['user_id','book_id','rating']] \n",
    "# when importing from a DF, you only need to specify the scale of the ratings.\n",
    "reader = surprise.Reader(rating_scale=(1,5)) \n",
    "#into surprise:\n",
    "data = surprise.Dataset.load_from_df(raw,reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprise lets you implement your own algorithm while still being able to use its very powerful tools for measuring error and choosing hyperparameters. \n",
    "To do so, we have to make a class inheriting from the AlgoBase class. We'll define it along with a fit and estimate method.\n",
    "The fit method uses Stochastic Gradient Descent to be able to estimate the best probabilistic matrix factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProbabilisticMatrixFactorization(surprise.AlgoBase):\n",
    "# Randomly initializes two Matrices, Stochastic Gradient Descent to be able to optimize the best factorization for ratings.\n",
    "    def __init__(self,learning_rate,num_epochs,num_factors):\n",
    "       # super(surprise.AlgoBase)\n",
    "        self.alpha = learning_rate #learning rate for Stochastic Gradient Descent\n",
    "        self.num_epochs = num_epochs\n",
    "        self.num_factors = num_factors\n",
    "    def fit(self,train):\n",
    "        #randomly initialize user/item factors from a Gaussian\n",
    "        P = np.random.normal(0,.1,(train.n_users,self.num_factors))\n",
    "        Q = np.random.normal(0,.1,(train.n_items,self.num_factors))\n",
    "        #print('fit')\n",
    "\n",
    "        for epoch in range(self.num_epochs):\n",
    "            for u,i,r_ui in train.all_ratings():\n",
    "                residual = r_ui - np.dot(P[u],Q[i])\n",
    "                temp = P[u,:] # we want to update them at the same time, so we make a temporary variable. \n",
    "                P[u,:] +=  self.alpha * residual * Q[i]\n",
    "                Q[i,:] +=  self.alpha * residual * temp \n",
    "\n",
    "                \n",
    "        self.P = P\n",
    "        self.Q = Q\n",
    "\n",
    "        self.trainset = train\n",
    "    \n",
    "    \n",
    "    def estimate(self,u,i):\n",
    "        #returns estimated rating for user u and item i. Prerequisite: Algorithm must be fit to training set.\n",
    "        #check to see if u and i are in the train set:\n",
    "        #print('gahh')\n",
    "\n",
    "        if self.trainset.knows_user(u) and self.trainset.knows_item(i):\n",
    "            print(u,i, '\\n','yep:', self.P[u],self.Q[i])\n",
    "            #return scalar product of P[u] and Q[i]\n",
    "            nanCheck = np.dot(self.P[u],self.Q[i])\n",
    "            \n",
    "            if np.isnan(nanCheck): #kalo hasil dot product-nya Nan\n",
    "                print(nanCheck)\n",
    "                return self.trainset.global_mean\n",
    "            else:\n",
    "                print(\"tidak\")\n",
    "                return np.dot(self.P[u,:],self.Q[i,:])\n",
    "        else:# if its not known we'll return the general average. \n",
    "            print('global mean')\n",
    "            return self.trainset.global_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets train the data on the whole dataset, just to gauge the effectiveness of the model we just made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Alg1 = ProbabilisticMatrixFactorization(learning_rate=0.05,num_epochs=4,num_factors=10)\n",
    "data1 = data.build_full_trainset()\n",
    "Alg1.fit(data1)\n",
    "print(raw.user_id.iloc[4],raw.book_id.iloc[4])\n",
    "Alg1.estimate(raw.user_id.iloc[0],raw.book_id.iloc[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(raw.user_id[0], raw.book_id[0])\n",
    "# Alg1.estimate(raw.user_id.iloc[0],raw.book_id.iloc[400])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another cool thing about Suprise is it allows you to use their built in GridSearch, which is inspired by sk-learn's GridSearchCV. Its a handy tool that allows us to put in a whole range of hyper-parameters, and picks the best one. \n",
    "\n",
    "Lets use it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = surprise.model_selection.GridSearchCV(ProbabilisticMatrixFactorization, param_grid={'learning_rate':[0.005,0.01],\n",
    "                                                                            'num_epochs':[5,10],\n",
    "                                                                            'num_factors':[10,20]},measures=['rmse', 'mae'], cv=2)\n",
    "gs.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('rsme: ',gs.best_score['rmse'],'mae: ',gs.best_score['mae'])\n",
    "best_params = gs.best_params['rmse']\n",
    "print('rsme: ',gs.best_params['rmse'],'mae: ',gs.best_params['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestVersion = ProbabilisticMatrixFactorization(learning_rate=best_params['learning_rate'],num_epochs=best_params['num_epochs'],num_factors=best_params['num_factors'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can use k-fold cross validation to evaluate the best model. \n",
    "# let the model train\n",
    "kSplit = surprise.model_selection.KFold(n_splits=10,shuffle=True)\n",
    "for train,test in kSplit.split(data):\n",
    "    bestVersion.fit(train)\n",
    "    prediction = bestVersion.test(test)\n",
    "    surprise.accuracy.rmse(prediction,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which performed even better than in Grid Search Above, with even 0.2 less Root Means Squared error. Probabalistic Matrix Factorization is powerful tool to use, and Surprise allows us to alleviate some of the pain of data wrangling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hasil = np.zeros([136])\n",
    "for i in range(136):\n",
    "    for j in range(9108):\n",
    "#     hasil[i] = \n",
    "        hasil = np.dot(bestVersion.P[j], bestVersion.Q[i])\n",
    "        if(hasil >= 5.0):\n",
    "            print(i, j, hasil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bestVersion.P.shape, bestVersion.Q.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestVersion.fit(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hasil = np.dot(bestVersion.P[8], bestVersion.Q[4])\n",
    "print(hasil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(bestVersion.P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# W (weight) = user rate feature vector\n",
    "with open('./data_result/W.csv', 'w', encoding=\"utf-8\") as f:\n",
    "\n",
    "    # using csv.writer method from CSV package\n",
    "    write = csv.writer(f)\n",
    "    write.writerows(bestVersion.P)\n",
    "\n",
    "# X = movie rate feature vector\n",
    "with open('./data_result/X.csv', 'w', encoding=\"utf-8\") as f:\n",
    "\n",
    "    # using csv.writer method from CSV package\n",
    "    write = csv.writer(f)\n",
    "    write.writerows(bestVersion.Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-f99966fae2bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "# https://youtu.be/HXzz87WVm6c\n",
    "\"\"\"\n",
    "Dataset from: https://lhncbc.nlm.nih.gov/publication/pub9932\n",
    "\n",
    "Binary problem: Question is: Is the cell in the image infected/parasited? \n",
    "If yes, probability is close to 1. \n",
    "If no, the probablility is close to 0. (uninfected)\n",
    "\n",
    "This is because we added label 1 to parasited images. \n",
    "In summary, probability result close to 1 reflects infected (parasited) image \n",
    "and close to 0 reflects uninfected image\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "#from keras import backend as K\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "#Test GPU\n",
    "# import tensorflow as tf\n",
    "# device_name = tf.test.gpu_device_name()\n",
    "# if device_name != '/device:GPU:0':\n",
    "#   raise SystemError('GPU device not found')\n",
    "# print('Found GPU at: {}'.format(device_name))\n",
    "\n",
    "image_directory = 'cell_images/'\n",
    "SIZE = 150\n",
    "dataset = []  #Many ways to handle data, you can use pandas. Here, we are using a list format.  \n",
    "label = []  #Place holders to define add labels. We will add 0 to all parasitized images and 1 to uninfected.\n",
    "\n",
    "\n",
    "parasitized_images = os.listdir(image_directory + 'Parasitized/')\n",
    "for i, image_name in enumerate(parasitized_images):    #Remember enumerate method adds a counter and returns the enumerate object\n",
    "    \n",
    "    if (image_name.split('.')[1] == 'png'):\n",
    "        image = cv2.imread(image_directory + 'Parasitized/' + image_name)\n",
    "        image = Image.fromarray(image, 'RGB')\n",
    "        image = image.resize((SIZE, SIZE))\n",
    "        dataset.append(np.array(image))\n",
    "        label.append(1)\n",
    "        \n",
    "uninfected_images = os.listdir(image_directory + 'Uninfected/')\n",
    "for i, image_name in enumerate(uninfected_images):\n",
    "    if (image_name.split('.')[1] == 'png'):\n",
    "        image = cv2.imread(image_directory + 'Uninfected/' + image_name)\n",
    "        image = Image.fromarray(image, 'RGB')\n",
    "        image = image.resize((SIZE, SIZE))\n",
    "        dataset.append(np.array(image))\n",
    "        label.append(0)\n",
    "        \n",
    "dataset = np.array(dataset)\n",
    "label = np.array(label)\n",
    "print(\"Dataset size is \", dataset.shape)\n",
    "print(\"Label size is \", label.shape)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset, label, test_size = 0.20, random_state = 0)\n",
    "print(\"Train size is \", X_train.shape)\n",
    "print(\"Test size is \", X_test.shape)\n",
    "\n",
    "from keras.utils import normalize\n",
    "X_train = normalize(X_train, axis=1)\n",
    "X_test = normalize(X_test, axis=1)\n",
    "\n",
    "###Define the model\n",
    "\n",
    "INPUT_SHAPE = (SIZE, SIZE, 3)   #change to (SIZE, SIZE, 3)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=INPUT_SHAPE))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), kernel_initializer = 'he_uniform'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), kernel_initializer = 'he_uniform'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))  \n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',             #also try adam\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary()) \n",
    "\n",
    "history = model.fit(X_train, \n",
    "                         y_train, \n",
    "                         batch_size = 64, \n",
    "                         verbose = 1, \n",
    "                         epochs = 100,      \n",
    "                         validation_data=(X_test,y_test),\n",
    "                         shuffle = False\n",
    "                     )\n",
    "\n",
    "\n",
    "model.save('models/malaria_model_100epochs.h5')  \n",
    "\n",
    "#plot the training and validation accuracy and loss at each epoch\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "plt.plot(epochs, acc, 'y', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "n=23  #Select the index of image to be loaded for testing\n",
    "img = X_test[n]\n",
    "plt.imshow(img)\n",
    "input_img = np.expand_dims(img, axis=0) #Expand dims so the input is (num images, x, y, c)\n",
    "print(\"The prediction for this image is: \", model.predict(input_img))\n",
    "print(\"The actual label for this image is: \", y_test[n])\n",
    "\n",
    "\n",
    "#We can load the trained model, so we don't have to train again for 300 epochs!\n",
    "from keras.models import load_model\n",
    "# load model\n",
    "model = load_model('models/malaria_model_100epochs.h5')\n",
    "\n",
    "#For 300 epochs, giving 82.5% accuracy\n",
    "\n",
    "_, acc = model.evaluate(X_test, y_test)\n",
    "print(\"Accuracy = \", (acc * 100.0), \"%\")\n",
    "\n",
    "\n",
    "mythreshold=0.885\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = (model.predict(X_test)>= mythreshold).astype(int)\n",
    "cm=confusion_matrix(y_test, y_pred)  \n",
    "print(cm)\n",
    "\n",
    "\n",
    "#ROC\n",
    "from sklearn.metrics import roc_curve\n",
    "y_preds = model.predict(X_test).ravel()\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_preds)\n",
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'y--')\n",
    "plt.plot(fpr, tpr, marker='.')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "i = np.arange(len(tpr)) \n",
    "roc = pd.DataFrame({'tf' : pd.Series(tpr-(1-fpr), index=i), 'thresholds' : pd.Series(thresholds, index=i)})\n",
    "ideal_roc_thresh = roc.iloc[(roc.tf-0).abs().argsort()[:1]]  #Locate the point where the value is close to 0\n",
    "print(\"Ideal threshold is: \", ideal_roc_thresh['thresholds']) \n",
    "\n",
    "\n",
    "from sklearn.metrics import auc\n",
    "auc_value = auc(fpr, tpr)\n",
    "print(\"Area under curve, AUC = \", auc_value)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
